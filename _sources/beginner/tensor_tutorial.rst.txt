.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_beginner_tensor_tutorial.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_beginner_tensor_tutorial.py:


Tensors
--------------------------------------------

Tensors are a specialized data structure that are very similar to arrays
and matrices. In PyTorch, we use tensors to encode the inputs and
outputs of a model, as well as the model’s parameters.

Tensors are similar to NumPy’s ndarrays, except that tensors can run on
a GPU to accelerate computing. If you’re familiar with ndarrays, you’ll
be right at home with the Tensor API. If not, follow along in this quick
API walkthrough.




.. code-block:: default


    import torch
    import numpy as np








Tensor Initialization
~~~~~~~~~~~~~~~~~~~~~

Tensors can be initialized in various ways. Take a look at the following examples

**Directly from data / NumPy arrays:**
Tensors can be created directly by passing a Python list or sequence using the ``torch.tensor()`` constructor. The data type is automatically inferred from the data.


.. code-block:: default


    data = [[1, 2],[3, 4]]
    np_array = np.array(data)

    tnsr_from_data = torch.tensor(data)
    tnsr_from_np = torch.from_numpy(np_array)








**From another tensor:**
The new tensor retains the properties of the arg tensor, unless explicitly overridden.


.. code-block:: default


    new_ones_tnsr = torch.ones_like(tnsr_from_data) # 2 x 2 matrix of ones

    try:
      new_rand_tnsr = torch.rand_like(tnsr_from_data) # 2 x 2 matrix of random numbers
    except RuntimeError as e:
      print(f"RuntimeError thrown: {e}")
      print()
      print(f"Random values in PyTorch are floating points. Datatype passed to torch.rand_like(): {tnsr_from_data.dtype}")






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    RuntimeError thrown: "check_uniform_bounds" not implemented for 'Long'

    Random values in PyTorch are floating points. Datatype passed to torch.rand_like(): torch.int64


This works after we override the dtype property



.. code-block:: default


    new_rand_tnsr = torch.rand_like(tnsr_from_data, dtype=torch.float)
    print(new_rand_tnsr)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    tensor([[0.4782, 0.0577],
            [0.2378, 0.2681]])


**With random or constant values:**



.. code-block:: default


    shape = (2,3,)
    rand_tnsr = torch.rand(shape)
    ones_tnsr = torch.ones(shape)
    zeros_tnsr = torch.zeros(shape)
    print(f"Random Tensor:\n{rand_tnsr} \n\n \
            Ones Tensor:\n{ones_tnsr} \n\n \
            Zeros Tensor:\n{zeros_tnsr}")








.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Random Tensor:
    tensor([[0.3376, 0.5569, 0.8947],
            [0.0201, 0.5931, 0.4131]]) 

             Ones Tensor:
    tensor([[1., 1., 1.],
            [1., 1., 1.]]) 

             Zeros Tensor:
    tensor([[0., 0., 0.],
            [0., 0., 0.]])


--------------


Tensor Attributes
~~~~~~~~~~~~~~~~~

Tensor attributes describe their shape data type and where they live.


.. code-block:: default


    tnsr = torch.rand(3,4)

    print(f"Shape of tensor: {tnsr.shape}")
    print(f"Datatype of tensor: {tnsr.dtype}")
    print(f"Device tensor lives on: {tnsr.device}")






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Shape of tensor: torch.Size([3, 4])
    Datatype of tensor: torch.float32
    Device tensor lives on: cpu


--------------


Tensor Operations
~~~~~~~~~~~~~~~~~

Over 100 tensor operations, including transposing, indexing, slicing,
mathematical operations, linear algebra, random sampling, and more are
comprehensively described
`here <https://pytorch.org/docs/stable/torch.html>`__.

Each of them can be run on the GPU (at typically higher speeds than on a
CPU). If you’re using Colab, allocate a GPU by going to Edit > Notebook
Settings.



.. code-block:: default


    # We move our tensor to the GPU if available
    if torch.cuda.is_available():
      tnsr = tnsr.to('cuda')








Try out some of the operations from the list. If you're familiar with the NumPy API, you'll find the Tensor API a breeze to use.


**Standard numpy-like indexing and slicing:**


.. code-block:: default


    tnsr = torch.ones(4, 4)
    tnsr[:,1] = 0
    print(tnsr)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    tensor([[1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.]])


**Both of these are joining ops, but they are subtly different.**


.. code-block:: default


    t1 = torch.cat([tnsr, tnsr], dim=1)
    t2 = torch.stack([tnsr, tnsr], dim=1)
    print(t1)
    print()
    print(t2)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    tensor([[1., 0., 1., 1., 1., 0., 1., 1.],
            [1., 0., 1., 1., 1., 0., 1., 1.],
            [1., 0., 1., 1., 1., 0., 1., 1.],
            [1., 0., 1., 1., 1., 0., 1., 1.]])

    tensor([[[1., 0., 1., 1.],
             [1., 0., 1., 1.]],

            [[1., 0., 1., 1.],
             [1., 0., 1., 1.]],

            [[1., 0., 1., 1.],
             [1., 0., 1., 1.]],

            [[1., 0., 1., 1.],
             [1., 0., 1., 1.]]])


**Multiply op - multiple syntaxes**


.. code-block:: default


    # both ops compute element-wise product
    print(tnsr * tnsr == tnsr.mul(tnsr))

    # both ops compute matrix product
    print(tnsr @ tnsr.T == tnsr.matmul(tnsr.T))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    tensor([[True, True, True, True],
            [True, True, True, True],
            [True, True, True, True],
            [True, True, True, True]])
    tensor([[True, True, True, True],
            [True, True, True, True],
            [True, True, True, True],
            [True, True, True, True]])


.. note::
    Operations that have a '_' suffix are in-place. For example:
    ``x.copy_(y)``, ``x.t_()``, will change ``x``. In-place operations
    don't work well with Autograd, and their use is discouraged.


.. code-block:: default


    print(tnsr)
    tnsr.t_()
    print(tnsr)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    tensor([[1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.]])
    tensor([[1., 1., 1., 1.],
            [0., 0., 0., 0.],
            [1., 1., 1., 1.],
            [1., 1., 1., 1.]])


--------------


Bridge with NumPy
~~~~~~~~~~~~~~~~~
Tensors on the CPU and NumPy arrays can share their underlying memory
locations, and changing one will change	the other.

Tensor to NumPy array
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


.. code-block:: default

    a = torch.ones(5)
    print(f"a: {a}")
    b = a.numpy()
    print(f"b: {b}")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    a: tensor([1., 1., 1., 1., 1.])
    b: [1. 1. 1. 1. 1.]


A change in ``a`` reflects in ``b``


.. code-block:: default


    a.add_(1)
    print(f"a: {a}")
    print(f"b: {b}")






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    a: tensor([2., 2., 2., 2., 2.])
    b: [2. 2. 2. 2. 2.]


NumPy array to Tensor
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


.. code-block:: default

    a = np.ones(5)
    b = torch.from_numpy(a)
    np.add(a, 1, out=a)
    print(f"a: {a}")
    print(f"b: {b}")




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    a: [2. 2. 2. 2. 2.]
    b: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.069 seconds)


.. _sphx_glr_download_beginner_tensor_tutorial.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: tensor_tutorial.py <tensor_tutorial.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: tensor_tutorial.ipynb <tensor_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
