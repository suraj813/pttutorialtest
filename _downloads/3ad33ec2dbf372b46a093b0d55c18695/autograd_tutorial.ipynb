{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nA Gentle Introduction to Autograd\n---------------------------------\n\nAutograd is PyTorch\u2019s automatic differentiation engine that powers\nneural network training. In this section, you will get a conceptual\nunderstanding of how autograd works under the hood.\n\nBackground\n~~~~~~~~~~\nNeural networks (NNs) are a collection of nested functions that are\nexecuted on some input data. These functions are defined by *parameters*\n(consisting of weights and biases), which in PyTorch are stored in\ntensors.\n\nTraining a NN happens in two steps:\n\n**Forward Propagation**: In forward prop, the NN makes its best guess\nabout the correct output. It runs the input data through each of its\nfunctions to make this guess.\n\n**Backward Propagation**: In backprop, the NN adjusts its parameters\nproportionate to the error in its guess. It does this by traversing\nbackwards from the output, collecting the derivatives of the error with\nrespect to the parameters of the functions (*gradients*), and optimizing\nthe parameters using **gradient descent**. For a more detailed walkthrough\nof backprop, check out this `video from\n3Blue1Brown <https://www.youtube.com/watch?v=tIeHLnjs5U8>`__.\n\nMost deep learning frameworks use automatic differentiation for\nbackprop; in PyTorch, it is handled by Autograd.\n\n\nUsage in PyTorch\n~~~~~~~~~~~\nBackward propagation can be kicked off by calling ``.backward()`` on the error tensor.\nThis collects the gradients for each parameter in the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n\nmodel = torchvision.models.resnet18(pretrained=True)\ndata = torch.rand(1, 3, 64, 64)\nlabels = torch.rand(1, 1000)\nprediction = model(data) # forward pass\nloss = (prediction - labels).sum()\nloss.backward() # backward pass\n\noptim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\noptim.step() #gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point, you have everything you need to build your neural network.\nThe below sections detail the workings of autograd - feel free to skip them.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Differentiation in Autograd\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The ``requires_grad`` flag lets autograd know\n# if we need gradients w.r.t. these tensors. If it is ``True``, autograd\n# tracks all operations on them.\n\n\nimport torch\n\na = torch.tensor([2., 3.], requires_grad=True)\nb = torch.tensor([6., 4.], requires_grad=True)\n\nQ = 3*a**3 - b**2\nprint(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``a`` and ``b`` can be viewed as parameters of an NN, with ``Q``\nanalogous to the error. In training we want gradients of the error\nw.r.t. parameters, i.e.\n\n\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n\n\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n\nSince ``Q`` is a vector, we pass a ``gradient`` argument in\n``.backward()``.\n\n``gradient`` is a tensor of the same shape. Here it represents the\ngradient of Q w.r.t. itself, i.e.\n\n\\begin{align}\\frac{dQ}{dQ} = 1\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "external_grad = torch.tensor([1., 1.])\nQ.backward(gradient=external_grad)\n\n# check if autograd's gradients are correct\nprint(9*a**2 == a.grad)\nprint(-2*b == b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optional Reading - Vector Calculus in Autograd\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nMathematically, if you have a vector valued function\n$\\vec{y}=f(\\vec{x})$, then the gradient of $\\vec{y}$ with\nrespect to $\\vec{x}$ is a Jacobian matrix $J$:\n\n\\begin{align}J\n     =\n      \\left(\\begin{array}{cc}\n      \\frac{\\partial \\bf{y}}{\\partial x_{1}} &\n      ... &\n      \\frac{\\partial \\bf{y}}{\\partial x_{n}}\n      \\end{array}\\right)\n     =\n     \\left(\\begin{array}{ccc}\n      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n      \\vdots & \\ddots & \\vdots\\\\\n      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n      \\end{array}\\right)\\end{align}\n\nGenerally speaking, ``torch.autograd`` is an engine for computing\nvector-Jacobian product. That is, given any vector $\\vec{v}$, compute the product\n$J^{T}\\cdot \\vec{v}$\n\nIf $v$ happens to be the gradient of a scalar function\n\n\\begin{align}l\n   =\n   g\\left(\\vec{y}\\right)\n   =\n   \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}\\end{align}\n\nthen by the chain rule, the vector-Jacobian product would be the\ngradient of $l$ with respect to $\\vec{x}$:\n\n\\begin{align}J^{T}\\cdot \\vec{v}=\\left(\\begin{array}{ccc}\n      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n      \\vdots & \\ddots & \\vdots\\\\\n      \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n      \\end{array}\\right)\\left(\\begin{array}{c}\n      \\frac{\\partial l}{\\partial y_{1}}\\\\\n      \\vdots\\\\\n      \\frac{\\partial l}{\\partial y_{m}}\n      \\end{array}\\right)=\\left(\\begin{array}{c}\n      \\frac{\\partial l}{\\partial x_{1}}\\\\\n      \\vdots\\\\\n      \\frac{\\partial l}{\\partial x_{n}}\n      \\end{array}\\right)\\end{align}\n\nThis characteristic of vector-Jacobian product is what we use in the above example;\n``external_grad`` represents $\\vec{v}$.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computational Graph\n~~~~~~~~~~~~~~~~~~~\n\nConceptually, autograd keeps a record of data (tensors) & all executed\noperations (along with the resulting new tensors) in a directed acyclic\ngraph (DAG) consisting of\n`Function <https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>`__\nobjects. In this DAG, leaves are the input tensors, roots are the output\ntensors. By tracing this graph from roots to leaves, you can\nautomatically compute the gradients using the chain rule.\n\nIn a forward pass, autograd does two things simultaneously: \\* run the\nrequested operation to compute a resulting tensor, and \\* maintain the\noperation\u2019s *gradient function* in the DAG. This is stored in the\nresulting tensor\u2019s .\\ ``grad_fn`` attribute.\n\nThe backward pass kicks off when ``.backward()`` is called on the DAG\nroot. Autograd then \\* computes the gradients from each ``.grad_fn``, \\*\naccumulates them in the respective tensor\u2019s ``.grad`` attribute, and \\*\nusing the chain rule, propagates all the way to the leaf tensors.\n\n.. Note::\n  **Autograd DAGs are dynamic in PyTorch**\n  An important thing to note is that the graph is recreated from scratch; after each\n  ``.backward()`` call, autograd starts populating a new graph. This is\n  exactly what allows you to use control flow statements in your model;\n  you can change the shape, size and operations at every iteration if\n  needed. Autograd does not need you to encode all possible paths\n  beforehand.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exclusion from the DAG\n^^^^^^^^^^^^^^^^^^^^^^\n\nAutograd tracks operations on all tensors which have their\n``requires_grad`` flag set to ``True``. For tensors that don\u2019t require\ngradients, setting this attribute to ``False`` excludes it from the\ngradient computation DAG and increases efficiency.\n\nThe output tensor of an operation will require gradients even if only a\nsingle input tensor has ``requires_grad=True``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.rand(5, 5)\ny = torch.rand(5, 5)\nz = torch.rand((5, 5), requires_grad=True)\n\na = x + y\nprint(\"Does `a` require gradients?\")\nprint(a.requires_grad==True)\nb = x + z\nprint(\"Does `b` require gradients?\")\nprint(b.requires_grad==True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is especially useful when you want to freeze part of your model\n(for instance, when you\u2019re fine-tuning a pretrained model), or you know\nin advance that you\u2019re not going to use gradients w.r.t. some\nparameters.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n\nmodel = torchvision.models.resnet18(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False\n# Replace the last fully-connected layer\n# Parameters of nn.Module instances have requires_grad=True by default\nmodel.fc = nn.Linear(512, 100)\n\n# Optimize only the classifier\noptimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same functionality is available as a context manager in\n`torch.no_grad() <https://pytorch.org/docs/stable/generated/torch.no_grad.html>`__\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Further readings:\n~~~~~~~~~~~~~~~~~~~\n\n-  `In-place operations & Multithreaded Autograd <https://pytorch.org/docs/stable/notes/autograd.html>`__\n-  `Example implementation of reverse-mode autodiff <https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC>`__\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}